# -*- coding: utf-8 -*-
"""Roberta_PreTrained_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-vzEiy_WF4-JS5yTnQjfK9YW5r9t6DG6
"""
import pickle

import googletrans
from googletrans import Translator

print(googletrans.LANGUAGES)

# Creating object of translator
translator = Translator(service_urls=['translate.googleapis.com'])
import re
import nltk
from nltk.corpus import stopwords
from transformers import AutoTokenizer
from scipy.special import softmax

# Query
query = "Bitcoin"

import snscrape.modules.twitter as sntwitter

tweetsList = []
limit = 100  # initial limit

for tweet in sntwitter.TwitterSearchScraper(query + " since:2021-01-01 until:2021-05-31").get_items():
    if limit == len(tweetsList):
        break
    else:
        tweetsList.append(tweet.content)

i = 1
for sentence in tweetsList:
    print(f"{i} :    {sentence}   ")
    i += 1

convertedTweetsList = []
for tweet in tweetsList:
    # auto translation from any language to english language
    result = translator.translate(tweet)
    # .translate(tweet)
    convertedTweetsList.append(result.text)

i = 1
for sentence in convertedTweetsList:
    print(f"{i} :    {sentence}   ")
    i += 1
# roberta = "cardiffnlp/twitter-roberta-base-sentiment"
# model = TFAutoModelForSequenceClassification.from_pretrained(roberta)

tokenizer = AutoTokenizer.from_pretrained(roberta)
# pickle.dump(model, open('model.pkl', 'wb'))
model = pickle.load(open("roberta_model.pkl", "rb"))

nltk.download('stopwords')
stop_words = set(stopwords.words("english"))


def removeStopWordsAndPuntuations(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = [w for w in text.split() if w not in stop_words]
    return " ".join(text)


labels = ['Negative', 'Neutral', 'Positive']

positive = 0
negative = 0
neutral = 0

for tweet in convertedTweetsList:
    cleanedText = removeStopWordsAndPuntuations(tweet)
    # print(cleanedText)
    textString = str(cleanedText)
    # sentiment analysis
    encoded_tweet = tokenizer(textString, return_tensors='pt')
    # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])
    output = model(**encoded_tweet)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)

    negScore = scores[0]
    neuScore = scores[1]
    posScore = scores[2]

    if max(posScore, neuScore, negScore) == posScore:
        positive += 1
    elif max(posScore, neuScore, negScore) == neuScore:
        neutral += 1
    elif max(posScore, neuScore, negScore) == negScore:
        negative += 1

"""# Searched KeyWord is 'Machine Learning'"""

print("searched keyword is : ", query, "\n")
print("Number of positive Tweets are  :  ", positive)
print("Number of Negative Tweets are  :  ", negative)
print("Number of Neutral Tweets are   :  ", neutral)
